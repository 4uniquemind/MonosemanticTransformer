Это модель представляет собой модификацию основанной на трансформерах модели генерации языка GPT-2. Инновация состоит в использовании промежуточного слоя, представляющего собой разреженный автоэнкодер, для изменения активаций промежуточных слоев в GPT-2. 

Автоэнкодер обучается декодировать входные данные в том же пространстве, в котором они изначально представлены, но с применением L1-регуляризации для поощрения разреженности активаций. Это может приводить к более интерпретируемым и значимым активациям, которые затем используются для дообучения финального слоя в GPT-2.

Этот подход может быть полезен для улучшения интерпретируемости и контролируемости моделей на основе трансформеров, таких как GPT-2.
